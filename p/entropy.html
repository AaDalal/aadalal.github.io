<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.ico" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		
		<link href="../_app/immutable/assets/0.B2qIdcSH.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.B4uZwZ_f.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BzZkoMtX.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/WjKqBSco.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BTykkvGJ.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.1xwXJt5M.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/z7WzBOfM.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CY9FYKcK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BOKyN92P.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.xI6WVgmj.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/oPucQqzi.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/3.DL91LGBd.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BFHx90Sw.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/Be_KLNhx.js"><!--xod7dh--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC" crossorigin="anonymous"/><!----><title>Entropy | aagam's blog</title>
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><!--[--><!----><main><div><article class="max-w-2xl mx-auto px-3 mt-4"><header><div class="min-h-20 py-2 flex items-end"><a href="/">← Back</a></div> <div class="dark:bg-orange-800 bg-orange-900/60"><div class="italic opacity-75 mb-2 ml-2 bg-transparent"><time>2021-12-28</time></div> <h1 class="text-8xl leading-none w-3/4 font-junicode mb-10 ml-1 bg-none">Entropy</h1></div></header> <section class="prose dark:prose-invert prose-a:text-blue-600 dark:prose-a:text-blue-500 hover:prose-a:text-blue-900 hover:dark:prose-a:text-blue-800 dark:prose-pre:border-gray-900 dark:prose-pre:border leading-1 first-letter:text-5xl first-letter:mr-2 first-letter:leading-none first-letter:float-left first-letter:border-solid first-letter:px-1"><!----><p>Inspired by <a href="https://www.youtube.com/watch?v=sMb00lz-IfE">this video</a> on compression, I wanted to understand what carrying information actually means, from a few interesting examples relating to repeated random events (like how much information is required to encode flipping a coin 100, 1000, or more times).</p>
<h1>Intuition for Entropy</h1>
<p>Shannon entropy is the expectation of the number of bits required to encode a particular symbol.</p>
<p>Imagine if you took a character out of a string like <code>"aaaaaaaaaaaa..."</code>. Since you know that every character is an <code>a</code>, there is actually very little information encoded (no information, if the length of the string has already been given). Similarly if you had a 99.9% chance of an a, there is still very little information encoded.</p>
<p>Therefore we can see that the entropy is inverse of the probability -- kind of. If the probability of an <code>a</code> gets really small, then you similarly dont need to encode it very often so its entropy becomes low. Really, we want a parabolic-looking function for the probability:</p>
<p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mi>i</mi><mi>n</mi></msubsup><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>i</mi></msub><mtext> s.t. </mtext><msubsup><mo>∑</mo><mi>i</mi><mi>n</mi></msubsup><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_i^n -p_i\log p_i \text{ s.t. } \sum_i^n p_i = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord text"><span class="mord"> s.t. </span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span></p>
<p>Here's a look at what the function inside the sigma looks like (using base 2). As <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p \to 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span></span> the linear p term dominates, while in the middle, the logarithmic term plays a big role:
<img src="/images/2021-12-27-21-04-14.png" alt=""></p>
<p>Here's maybe a more useful view. Red is function we are examining, green is a linear function and blue is a negative log:
<img src="/images/2021-12-27-21-15-00.png" alt=""></p>
<p>One thing that might be confusing is how exactly the log of base 2 related to the number of bits of information. For a second imagine that we use base 10 instead of base 2. Now imagine the probability was .1, then .01, .001 etc. If we wanted to encode that these were the probabilities, then we would need to use <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>1</mn></msub><mn>0</mn><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">-\log_10(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span></span> digits. It's a similar thing for base 2, if we now consider numbers encoded in binary. Note that this isn't so much about the <em>precision</em> of the probability as it is about the magnitude; if we literally wanted to encode the exact probability and it were .100000000000000001 then we might need to use many more digits than the entropy predicts, even though it makes little difference.</p>
<p>Note that non-power of 2 probabilities, this explanation implies fractional binary digits, but we can accept this as the nature of such a metric.</p>
<h1>Applying entropy</h1>
<p>It's useful to think of where we can find entropy both in the natural sciences and in statistics.</p>
<h2>The link to entropy in science</h2>
<p>Entropy in physics, for example, is often stated as the number of microstates a system can achieve, ie all the ways the particles can be arranged etc. This is true, but there is also a probability component -- as probability diminishes to 0, the entropy a microstate contributes is small (it's so unlikely to occur). This mirrors the intuition for information theoretical entropy.</p>
<h2>Entropy of the number of successes with the number of trials? (Binomial)</h2>
<p>Let X be the binomial random variable that denotes the number of successes of n bernouli trials. The probability mass function is given by <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy="false">[</mo><mi>X</mi><mo>=</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><msup><mi>p</mi><mi>i</mi></msup><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><msup><mo stretchy="false">)</mo><mrow><mi>n</mi><mo>−</mo><mi>i</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Pr[X = i] = p^i(1-p)^{n-i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0747em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0747em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></span></span>. We aim to find <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mi>p</mi><mi>i</mi></msup><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><msup><mo stretchy="false">)</mo><mrow><mi>n</mi><mo>−</mo><mi>i</mi></mrow></msup><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msup><mi>p</mi><mi>i</mi></msup><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><msup><mo stretchy="false">)</mo><mrow><mi>n</mi><mo>−</mo><mi>i</mi></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\sum_{i = 1}^{n} p^i(1-p)^{n-i} \log(p^i(1-p)^{n-i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1244em;vertical-align:-0.2997em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0747em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0747em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>.</p>
<p>Here's <a href="https://www.desmos.com/calculator/hsxokrr2to">a desmos</a> with the function.</p>
<p>We see some interesting behavior. For a small number of trials, the entropy is small, but increases then rapidly drops to 0. The fact that the entropy rises for the first couple trials illustrates that entropy rises as more cases are possible (you can have a greater range of values in a binomial random variable of 2 trials than on 1 trial). The fact that it eventually limits to 0 shows us the value of repeated trials in increasing our certainty and thereby reducing entropy.</p><!----></section> <!--[!--><!--]--> <!--[!--><!--]--></article></div><!----></main><!----><!--]--><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1qo9mjh = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.B4uZwZ_f.js"),
						import("../_app/immutable/entry/app.1xwXJt5M.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 3],
							data: [null,{type:"data",data:{post:{title:"Entropy",date:"2021-12-28",slug:"entropy",content:"\u003Cp>Inspired by \u003Ca href=\"https://www.youtube.com/watch?v=sMb00lz-IfE\">this video\u003C/a> on compression, I wanted to understand what carrying information actually means, from a few interesting examples relating to repeated random events (like how much information is required to encode flipping a coin 100, 1000, or more times).\u003C/p>\n\u003Ch1>Intuition for Entropy\u003C/h1>\n\u003Cp>Shannon entropy is the expectation of the number of bits required to encode a particular symbol.\u003C/p>\n\u003Cp>Imagine if you took a character out of a string like \u003Ccode>\"aaaaaaaaaaaa...\"\u003C/code>. Since you know that every character is an \u003Ccode>a\u003C/code>, there is actually very little information encoded (no information, if the length of the string has already been given). Similarly if you had a 99.9% chance of an a, there is still very little information encoded.\u003C/p>\n\u003Cp>Therefore we can see that the entropy is inverse of the probability -- kind of. If the probability of an \u003Ccode>a\u003C/code> gets really small, then you similarly dont need to encode it very often so its entropy becomes low. Really, we want a parabolic-looking function for the probability:\u003C/p>\n\u003Cp>\u003Cspan class=\"math math-inline\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsubsup>\u003Cmo>∑\u003C/mo>\u003Cmi>i\u003C/mi>\u003Cmi>n\u003C/mi>\u003C/msubsup>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmi>p\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmi>log\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmsub>\u003Cmi>p\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmtext> s.t. \u003C/mtext>\u003Cmsubsup>\u003Cmo>∑\u003C/mo>\u003Cmi>i\u003C/mi>\u003Cmi>n\u003C/mi>\u003C/msubsup>\u003Cmsub>\u003Cmi>p\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo>=\u003C/mo>\u003Cmn>1\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\sum_i^n -p_i\\log p_i \\text{ s.t. } \\sum_i^n p_i = 1\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.104em;vertical-align:-0.2997em;\">\u003C/span>\u003Cspan class=\"mop\">\u003Cspan class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8043em;\">\u003Cspan style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.2029em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">n\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2997em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">−\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">lo\u003Cspan style=\"margin-right:0.01389em;\">g\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mord text\">\u003Cspan class=\"mord\"> s.t. \u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">\u003Cspan class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8043em;\">\u003Cspan style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.2029em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">n\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2997em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/p>\n\u003Cp>Here's a look at what the function inside the sigma looks like (using base 2). As \u003Cspan class=\"math math-inline\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>p\u003C/mi>\u003Cmo>→\u003C/mo>\u003Cmn>0\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">p \\to 0\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">→\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> the linear p term dominates, while in the middle, the logarithmic term plays a big role:\n\u003Cimg src=\"/images/2021-12-27-21-04-14.png\" alt=\"\">\u003C/p>\n\u003Cp>Here's maybe a more useful view. Red is function we are examining, green is a linear function and blue is a negative log:\n\u003Cimg src=\"/images/2021-12-27-21-15-00.png\" alt=\"\">\u003C/p>\n\u003Cp>One thing that might be confusing is how exactly the log of base 2 related to the number of bits of information. For a second imagine that we use base 10 instead of base 2. Now imagine the probability was .1, then .01, .001 etc. If we wanted to encode that these were the probabilities, then we would need to use \u003Cspan class=\"math math-inline\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmo>−\u003C/mo>\u003Cmsub>\u003Cmrow>\u003Cmi>log\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003C/mrow>\u003Cmn>1\u003C/mn>\u003C/msub>\u003Cmn>0\u003C/mn>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>p\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">-\\log_10(p)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">\u003Cspan class=\"mop\">lo\u003Cspan style=\"margin-right:0.01389em;\">g\u003C/span>\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.207em;\">\u003Cspan style=\"top:-2.4559em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">1\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2441em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span> digits. It's a similar thing for base 2, if we now consider numbers encoded in binary. Note that this isn't so much about the \u003Cem>precision\u003C/em> of the probability as it is about the magnitude; if we literally wanted to encode the exact probability and it were .100000000000000001 then we might need to use many more digits than the entropy predicts, even though it makes little difference.\u003C/p>\n\u003Cp>Note that non-power of 2 probabilities, this explanation implies fractional binary digits, but we can accept this as the nature of such a metric.\u003C/p>\n\u003Ch1>Applying entropy\u003C/h1>\n\u003Cp>It's useful to think of where we can find entropy both in the natural sciences and in statistics.\u003C/p>\n\u003Ch2>The link to entropy in science\u003C/h2>\n\u003Cp>Entropy in physics, for example, is often stated as the number of microstates a system can achieve, ie all the ways the particles can be arranged etc. This is true, but there is also a probability component -- as probability diminishes to 0, the entropy a microstate contributes is small (it's so unlikely to occur). This mirrors the intuition for information theoretical entropy.\u003C/p>\n\u003Ch2>Entropy of the number of successes with the number of trials? (Binomial)\u003C/h2>\n\u003Cp>Let X be the binomial random variable that denotes the number of successes of n bernouli trials. The probability mass function is given by \u003Cspan class=\"math math-inline\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>P\u003C/mi>\u003Cmi>r\u003C/mi>\u003Cmo stretchy=\"false\">[\u003C/mo>\u003Cmi>X\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmi>i\u003C/mi>\u003Cmo stretchy=\"false\">]\u003C/mo>\u003Cmo>=\u003C/mo>\u003Cmsup>\u003Cmi>p\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msup>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo>−\u003C/mo>\u003Cmi>p\u003C/mi>\u003Cmsup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmrow>\u003Cmi>n\u003C/mi>\u003Cmo>−\u003C/mo>\u003Cmi>i\u003C/mi>\u003C/mrow>\u003C/msup>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">Pr[X = i] = p^i(1-p)^{n-i}\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r\u003C/span>\u003Cspan class=\"mopen\">[\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">i\u003C/span>\u003Cspan class=\"mclose\">]\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0747em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8247em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0747em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"mclose\">\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8247em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">n\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>. We aim to find \u003Cspan class=\"math math-inline\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmsubsup>\u003Cmo>∑\u003C/mo>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmn>1\u003C/mn>\u003C/mrow>\u003Cmi>n\u003C/mi>\u003C/msubsup>\u003Cmsup>\u003Cmi>p\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msup>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo>−\u003C/mo>\u003Cmi>p\u003C/mi>\u003Cmsup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmrow>\u003Cmi>n\u003C/mi>\u003Cmo>−\u003C/mo>\u003Cmi>i\u003C/mi>\u003C/mrow>\u003C/msup>\u003Cmi>log\u003C/mi>\u003Cmo>⁡\u003C/mo>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsup>\u003Cmi>p\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msup>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmn>1\u003C/mn>\u003Cmo>−\u003C/mo>\u003Cmi>p\u003C/mi>\u003Cmsup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmrow>\u003Cmi>n\u003C/mi>\u003Cmo>−\u003C/mo>\u003Cmi>i\u003C/mi>\u003C/mrow>\u003C/msup>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\sum_{i = 1}^{n} p^i(1-p)^{n-i} \\log(p^i(1-p)^{n-i})\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.1244em;vertical-align:-0.2997em;\">\u003C/span>\u003Cspan class=\"mop\">\u003Cspan class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8043em;\">\u003Cspan style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003Cspan class=\"mrel mtight\">=\u003C/span>\u003Cspan class=\"mord mtight\">1\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.2029em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">n\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2997em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8247em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0747em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"mclose\">\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8247em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">n\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop\">lo\u003Cspan style=\"margin-right:0.01389em;\">g\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8247em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">1\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">−\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0747em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">p\u003C/span>\u003Cspan class=\"mclose\">\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.8247em;\">\u003Cspan style=\"top:-3.063em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">n\u003C/span>\u003Cspan class=\"mbin mtight\">−\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>.\u003C/p>\n\u003Cp>Here's \u003Ca href=\"https://www.desmos.com/calculator/hsxokrr2to\">a desmos\u003C/a> with the function.\u003C/p>\n\u003Cp>We see some interesting behavior. For a small number of trials, the entropy is small, but increases then rapidly drops to 0. The fact that the entropy rises for the first couple trials illustrates that entropy rises as more cases are possible (you can have a greater range of values in a binomial random variable of 2 trials than on 1 trial). The fact that it eventually limits to 0 shows us the value of repeated trials in increasing our certainty and thereby reducing entropy.\u003C/p>",tags:"Information Theory"},allPosts:[{title:"Go fast!",slug:"fast",date:"2024-01-01"},{title:"Consistent Hashing",slug:"consistent-hashing",date:"2023-10-26"},{title:"Rendevous Hashing",slug:"rendevous-hashing",date:"2023-10-26"},{title:"Python Coroutines!?!?",slug:"py-generator-couroutines",date:"2023-06-11"},{title:"Lessons from Product Managing",slug:"PMing",date:"2023-06-01"},{title:"Einsum",slug:"einsum",date:"2022-03-14"},{title:"Entropy",slug:"entropy",date:"2021-12-28"},{title:"Try again with sudo: _sudo",slug:"_sudo",date:"2021-11-11"},{title:"Roku & Competition",slug:"competition-focus",date:"2021-11-11"}]},uses:{params:["slug"]}}],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
